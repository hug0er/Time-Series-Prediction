{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Short and sweet LSTM implementation in Tensorflow.\n",
    "\n",
    "Motivation:\n",
    "When Tensorflow was released, adding RNNs was a bit of a hack - it required\n",
    "building separate graphs for every number of timesteps and was a bit obscure\n",
    "to use. Since then TF devs added things like `dynamic_rnn`, `scan` and `map_fn`.\n",
    "Currently the APIs are decent, but all the tutorials that I am aware of are not\n",
    "making the best use of the new APIs.\n",
    "\n",
    "Advantages of this implementation:\n",
    "- No need to specify number of timesteps ahead of time. Number of timesteps is\n",
    "  infered from shape of input tensor. Can use the same graph for multiple\n",
    "  different numbers of timesteps.\n",
    "- No need to specify batch size ahead of time. Batch size is infered from shape\n",
    "  of input tensor. Can use the same graph for multiple different batch sizes.\n",
    "- Easy to swap out different recurrent gadgets (RNN, LSTM, GRU, your new\n",
    "  creative idea)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "map_fn = tf.map_fn\n",
    "\n",
    "################################################################################\n",
    "##                           DATASET GENERATION                               ##\n",
    "##                                                                            ##\n",
    "##  The problem we are trying to solve is adding two binary numbers. The      ##\n",
    "##  numbers are reversed, so that the state of RNN can add the numbers        ##\n",
    "##  perfectly provided it can learn to store carry in the state. Timestep t   ##\n",
    "##  corresponds to bit len(number) - t.                                       ##\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesData():\n",
    "    def __init__(self,num_points,xmin,xmax):\n",
    "        self.xmin=xmin\n",
    "        self.xmax=xmax\n",
    "        self.num_points=num_points\n",
    "        self.resolution=(xmax-xmin)/num_points\n",
    "        self.x_data=np.linspace(xmin,xmax,num_points)\n",
    "        self.y_true=np.sin(self.x_data)\n",
    "        \n",
    "    def ret_true(self,x_series):\n",
    "        return np.sin(x_series)\n",
    "    \n",
    "    def next_batch(self,batch_size,steps,return_batch_ts=False):\n",
    "        #Random starting point for batch\n",
    "        random_start=np.random.rand(batch_size,1)\n",
    "        \n",
    "        #Put the random point on the time series\n",
    "        ts_start=random_start*(self.xmax-self.xmin-(steps*self.resolution)) #Trick to convert any number into a point in the time series\n",
    "        \n",
    "        \n",
    "        batch_ts=ts_start+np.arange(0.0,steps+1) * self.resolution #steps +1 because we are predicting just one tme step ahead\n",
    "        \n",
    "        y_batch=np.sin(batch_ts)\n",
    "        \n",
    "        #Formatting for RNN\n",
    "        if return_batch_ts:\n",
    "            return y_batch[:,:-1].reshape(-1,steps,1) , y_batch[-1,-1].reshape(-1,1,1), batch_ts\n",
    "        else:\n",
    "            return y_batch[:,:-1].reshape(-1,steps,1) , y_batch[-1,-1].reshape(-1,1,1) #Returning the sequence shifted over one time step\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0.99151151],\n",
       "         [0.98377414],\n",
       "         [0.97357785],\n",
       "         [0.96094812],\n",
       "         [0.94591652],\n",
       "         [0.92852062],\n",
       "         [0.90880391],\n",
       "         [0.88681565],\n",
       "         [0.86261082],\n",
       "         [0.83624992]]]), array([[[0.80779882]]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable rnn/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\hug0er\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Users\\hug0er\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\hug0er\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-21b123f2a69c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m#  - outputs: (time, batch, output_size)  [do not mistake with OUTPUT_SIZE]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m#  - states:  (time, batch, hidden_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mrnn_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_major\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m# project output from rnn output size to OUTPUT_SIZE. Sometimes it is worth adding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[1;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m         dtype=dtype)\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m     \u001b[1;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[1;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 872\u001b[1;33m       swap_memory=swap_memory)\n\u001b[0m\u001b[0;32m    873\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m   \u001b[1;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   3289\u001b[0m       \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3290\u001b[0m     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,\n\u001b[1;32m-> 3291\u001b[1;33m                                     return_same_structure)\n\u001b[0m\u001b[0;32m   3292\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3293\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[1;34m(self, pred, body, loop_vars, shape_invariants, return_same_structure)\u001b[0m\n\u001b[0;32m   3002\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3003\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[1;32m-> 3004\u001b[1;33m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[0;32m   3005\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3006\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[1;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[0;32m   2937\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[0;32m   2938\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2939\u001b[1;33m     \u001b[0mbody_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2940\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2941\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(i, lv)\u001b[0m\n\u001b[0;32m   3258\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[0;32m   3259\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[1;32m-> 3260\u001b[1;33m         \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3262\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[1;34m(time, output_ta_t, state)\u001b[0m\n\u001b[0;32m    838\u001b[0m           skip_conditionals=True)\n\u001b[0;32m    839\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m       \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m     \u001b[1;31m# Keras cells always wrap state as list, even if it's a single tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    824\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_keras_rnn_cell\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 826\u001b[1;33m     \u001b[0mcall_cell\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, state, scope, *args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[1;31m# method.  See the class docstring for more details.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m     return base_layer.Layer.__call__(self, inputs, state, scope=scope,\n\u001b[1;32m--> 370\u001b[1;33m                                      *args, **kwargs)\n\u001b[0m\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m       \u001b[1;31m# Actually call layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    744\u001b[0m           \u001b[1;31m# the user has manually overwritten the build method do we need to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m           \u001b[1;31m# build it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m         \u001b[1;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m         \u001b[1;31m# constrained to set self.built.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(instance, input_shape)\u001b[0m\n\u001b[0;32m    147\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, inputs_shape)\u001b[0m\n\u001b[0;32m    714\u001b[0m     self._kernel = self.add_variable(\n\u001b[0;32m    715\u001b[0m         \u001b[0m_WEIGHTS_VARIABLE_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m         shape=[input_depth + h_depth, 4 * self._num_units])\n\u001b[0m\u001b[0;32m    717\u001b[0m     self._bias = self.add_variable(\n\u001b[0;32m    718\u001b[0m         \u001b[0m_BIAS_VARIABLE_NAME\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_variable\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0madd_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;34m\"\"\"Alias for `add_weight`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 495\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_subclass_implementers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[0;32m    286\u001b[0m             \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[0maggregation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m             getter=vs.get_variable)\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 609\u001b[1;33m         aggregation=aggregation)\n\u001b[0m\u001b[0;32m    610\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[1;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[0;32m    637\u001b[0m     new_variable = getter(\n\u001b[0;32m    638\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m         **kwargs_for_getter)\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m     \u001b[1;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1485\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1487\u001b[1;33m       aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1235\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1236\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1237\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m   1238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    538\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 540\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    490\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m           aggregation=aggregation)\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;31m# Set trainable value based on synchronization value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    859\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 861\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    862\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable rnn/basic_lstm_cell/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\hug0er\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"C:\\Users\\hug0er\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\hug0er\\Anaconda3\\envs\\Tesis\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "##                           GRAPH DEFINITION                                 ##\n",
    "################################################################################\n",
    "\n",
    "serie         =      TimeSeriesData(200,0,10)\n",
    "INPUT_SIZE    = 10       # 2 bits per timestep\n",
    "RNN_HIDDEN    = 100\n",
    "OUTPUT_SIZE   = 1       # 1 bit per timestep\n",
    "TINY          = 1e-6    # to avoid NaNs in logs\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "USE_LSTM = True\n",
    "\n",
    "inputs  = tf.placeholder(tf.float32, (None, None, INPUT_SIZE))  # (time, batch, in)\n",
    "outputs = tf.placeholder(tf.float32, (None, None, OUTPUT_SIZE)) # (time, batch, out)\n",
    "\n",
    "\n",
    "## Here cell can be any function you want, provided it has two attributes:\n",
    "#     - cell.zero_state(batch_size, dtype)- tensor which is an initial value\n",
    "#                                           for state in __call__\n",
    "#     - cell.__call__(input, state) - function that given input and previous\n",
    "#                                     state returns tuple (output, state) where\n",
    "#                                     state is the state passed to the next\n",
    "#                                     timestep and output is the tensor used\n",
    "#                                     for infering the output at timestep. For\n",
    "#                                     example for LSTM, output is just hidden,\n",
    "#                                     but state is memory + hidden\n",
    "# Example LSTM cell with learnable zero_state can be found here:\n",
    "#    https://gist.github.com/nivwusquorum/160d5cf7e1e82c21fad3ebf04f039317\n",
    "if USE_LSTM:\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(RNN_HIDDEN, state_is_tuple=True)\n",
    "else:\n",
    "    cell = tf.nn.rnn_cell.BasicRNNCell(RNN_HIDDEN)\n",
    "\n",
    "# Create initial state. Here it is just a constant tensor filled with zeros,\n",
    "# but in principle it could be a learnable parameter. This is a bit tricky\n",
    "# to do for LSTM's tuple state, but can be achieved by creating two vector\n",
    "# Variables, which are then tiled along batch dimension and grouped into tuple.\n",
    "batch_size    = tf.shape(inputs)[1]\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "# Given inputs (time, batch, input_size) outputs a tuple\n",
    "#  - outputs: (time, batch, output_size)  [do not mistake with OUTPUT_SIZE]\n",
    "#  - states:  (time, batch, hidden_size)\n",
    "rnn_outputs, rnn_states = tf.nn.dynamic_rnn(cell, inputs, initial_state=initial_state, time_major=True)\n",
    "\n",
    "# project output from rnn output size to OUTPUT_SIZE. Sometimes it is worth adding\n",
    "# an extra layer here.\n",
    "final_projection = lambda x: layers.linear(x, num_outputs=OUTPUT_SIZE, activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "# apply projection to every timestep.\n",
    "predicted_outputs = map_fn(final_projection, rnn_outputs)\n",
    "\n",
    "# compute elementwise cross entropy.\n",
    "error = -(outputs * tf.log(predicted_outputs + TINY) + (1.0 - outputs) * tf.log(1.0 - predicted_outputs + TINY))\n",
    "error = tf.reduce_mean(error)\n",
    "\n",
    "# optimize\n",
    "train_fn = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(error)\n",
    "\n",
    "# assuming that absolute difference between output and correct answer is 0.5\n",
    "# or less we can round it to the correct output.\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.abs(outputs - predicted_outputs) < 0.5, tf.float32))\n",
    "\n",
    "\n",
    "################################################################################\n",
    "##                           TRAINING LOOP                                    ##\n",
    "################################################################################\n",
    "\n",
    "NUM_BITS = 10\n",
    "ITERATIONS_PER_EPOCH = 100\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "valid_x, valid_y = generate_batch(num_bits=NUM_BITS, batch_size=100)\n",
    "\n",
    "session = tf.Session()\n",
    "# For some reason it is our job to do this:\n",
    "session.run(tf.initialize_all_variables())\n",
    "\n",
    "for epoch in range(1000):\n",
    "    epoch_error = 0\n",
    "    for _ in range(ITERATIONS_PER_EPOCH):\n",
    "        # here train_fn is what triggers backprop. error and accuracy on their\n",
    "        # own do not trigger the backprop.\n",
    "        x, y = generate_batch(num_bits=NUM_BITS, batch_size=BATCH_SIZE)\n",
    "        epoch_error += session.run([error, train_fn], {\n",
    "            inputs: x,\n",
    "            outputs: y,\n",
    "        })[0]\n",
    "    epoch_error /= ITERATIONS_PER_EPOCH\n",
    "    valid_accuracy = session.run(accuracy, {\n",
    "        inputs:  valid_x,\n",
    "        outputs: valid_y,\n",
    "    })\n",
    "    print (\"Epoch %d, train error: %.2f, valid accuracy: %.1f %%\" % (epoch, epoch_error, valid_accuracy * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
